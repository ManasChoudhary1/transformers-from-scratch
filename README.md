## Note :
Currently am working on a kaggle competition, so will continue to build this when done with that.

# Transformers From Scratch
Building Transformers and their components from the ground up.

This repository contains complete, working Transformer pipelines implemented from scratch. The goal is to explore the implementation details of the architecture by manually coding interesting variations of core componentsâ€”such as Multi-Head Attention, KV Caching, Multi-Query Attention, and Multi-Head Latent Attention.

# Structure
Each component includes:
**Notebooks**: Demos and theoretical explanations.
**Python Modules**: Modular code intended for integration into full models.

### Currently Implemented

1. Attention:

- Self-Attention
- Multi-Head Attention
- Multi-Head Attention (with KV Cache)
