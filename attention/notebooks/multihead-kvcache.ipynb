{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d2d046c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0979fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 1\n",
    "n = 5\n",
    "din = 8\n",
    "dout = 4\n",
    "n_heads = 2\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9482692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadkvcache(nn.Module):\n",
    "    def __init__(self,din,dout,n_heads):\n",
    "        super(multiheadkvcache,self).__init__()\n",
    "        self.din = din\n",
    "        self.dout = dout\n",
    "        self.n_heads = n_heads\n",
    "        self.dheads = int(dout/n_heads)\n",
    "        self.query = nn.Linear(din,dout)\n",
    "        self.key = nn.Linear(din,dout)\n",
    "        self.value = nn.Linear(din,dout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.out_proj = nn.Linear(dout,dout)\n",
    "        self.register_buffer(\"cachedk\",None,persistent=False)\n",
    "        self.register_buffer(\"cachedv\", None,persistent=False)\n",
    "        self.register_buffer(\"mask\", None,persistent=False)\n",
    "    def forward(self,X,training = False):\n",
    "        batch_size,n,din = X.shape\n",
    "        # getting the q,k,v matrices\n",
    "        q = self.query(X).view(batch_size,n,self.dout)\n",
    "        k = self.key(X).view(batch_size,n,self.dout)\n",
    "        v = self.value(X).view(batch_size,n,self.dout) ## (batch_size,n,din)\n",
    "        # splitting accross heads\n",
    "        q = q.view(batch_size,n,self.n_heads, self.dheads)\n",
    "        k = k.view(batch_size,n,self.n_heads, self.dheads)\n",
    "        v = v.view(batch_size,n,self.n_heads, self.dheads)\n",
    "        # if training is true we use cached K and V\n",
    "        if training is False:\n",
    "            if self.cachedk is None:\n",
    "                self.cachedk = k\n",
    "                self.cachedv = v\n",
    "            else :\n",
    "                self.cachedk = torch.concat([self.cachedk,k], dim = 1)\n",
    "                self.cachedv = torch.concat([self.cachedv,v], dim = 1)\n",
    "                k = self.cachedk\n",
    "                v = self.cachedv\n",
    "            # this is to prevent breaking the graph structure \n",
    "            self.cached_k = k.detach()\n",
    "            self.cached_v = v.detach()\n",
    "\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        k = k.transpose(2,3)\n",
    "        attn_scores = q @ k\n",
    "        attn_scores /= (self.dheads**0.5)\n",
    "        if training is True:\n",
    "            if self.mask is None:\n",
    "                curr_device = X.device\n",
    "                self.mask = torch.triu(torch.ones(n,n,device = curr_device),diagonal=1).bool()\n",
    "            attn_scores = torch.masked_fill(attn_scores,self.mask,-torch.inf)\n",
    "        attn_weights = self.softmax(attn_scores)\n",
    "        context_vector = attn_weights @ v\n",
    "\n",
    "        context_vector = context_vector.view(batch_size,self.n_heads,n,self.dheads).transpose(1,2).reshape(batch_size,n,self.dout)\n",
    "\n",
    "        return self.out_proj(context_vector)\n",
    "    def reset_cache(self):\n",
    "        self.cachedk = None\n",
    "        self.cachedv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c3420070",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.randn(batch_size,n,din)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e8924c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_cached = multiheadkvcache(din,dout,n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ac6d6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "93a3a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_cached.reset_cache()\n",
    "for itr in range(embeddings.shape[1]):\n",
    "        context_vectors.append([multihead_cached(embeddings[:,itr:itr+1,:],False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1d061695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 0.2977,  0.0681,  0.2475, -0.1201]]], grad_fn=<ViewBackward0>)]\n",
      "[tensor([[[ 0.1677, -0.1483,  0.6802, -0.0858]]], grad_fn=<ViewBackward0>)]\n",
      "[tensor([[[ 0.0659, -0.1886,  0.7839, -0.0063]]], grad_fn=<ViewBackward0>)]\n",
      "[tensor([[[ 0.1295, -0.0964,  0.5233, -0.0276]]], grad_fn=<ViewBackward0>)]\n",
      "[tensor([[[ 0.1684, -0.0374,  0.4336, -0.0415]]], grad_fn=<ViewBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "for val in context_vectors:\n",
    "    print((val))\n",
    "# we can see we get context vectors one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "53203c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_context_vector = multihead_cached(embeddings, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "04a95d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2977,  0.0681,  0.2475, -0.1201],\n",
      "         [ 0.1677, -0.1483,  0.6802, -0.0858],\n",
      "         [ 0.0659, -0.1886,  0.7839, -0.0063],\n",
      "         [ 0.1295, -0.0964,  0.5233, -0.0276],\n",
      "         [ 0.1684, -0.0374,  0.4336, -0.0415]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(training_context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f22e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
